from typing import TypedDict, Annotated, Sequence, List, Dict, Any
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_ollama import ChatOllama
from langgraph.graph import Graph, StateGraph
import json
import time

# å®šä¹‰çŠ¶æ€ç±»å‹
class AgentState(TypedDict):
    messages: Sequence[BaseMessage]
    current_step: str
    analysis: Dict[str, Any]
    next: str

# åˆ›å»ºèŠå¤©æ¨¡å‹
def create_chat_model(retries=3, delay=2):
    """åˆ›å»ºèŠå¤©æ¨¡å‹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    for attempt in range(retries):
        try:
            model = ChatOllama(
                # model="qwen2.5:14b",  # ä½¿ç”¨åŸºç¡€æ¨¡å‹
                model="deepseek-r1:70b",  # ä½¿ç”¨åŸºç¡€æ¨¡å‹
                base_url="http://localhost:11434",
                temperature=0.7,
                timeout=30
            )
            # æµ‹è¯•è¿æ¥
            test_prompt = "ç®€å•æµ‹è¯•"
            response = model.invoke(test_prompt)
            print("âœ“ Ollama æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model
        except Exception as e:
            if attempt < retries - 1:
                print(f"âš ï¸ å°è¯•è¿æ¥ Ollama å¤±è´¥ ({attempt + 1}/{retries}): {str(e)}")
                print(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                time.sleep(delay)
            else:
                raise RuntimeError(
                    "âŒ Ollama è¿æ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿ï¼š\n"
                    "1. Ollama æœåŠ¡å·²å¯åŠ¨ (ollama serve)\n"
                    "2. æ¨¡å‹å·²ä¸‹è½½ (ollama pull qwen:7b)\n"
                    "3. æœåŠ¡ç«¯å£ 11434 å¯è®¿é—®"
                )

# åˆ›å»ºæ¨¡å‹å®ä¾‹
try:
    model = create_chat_model()
except Exception as e:
    print(str(e))
    exit(1)

def safe_invoke(prompt: str, default_response: Dict = None) -> Dict:
    """å®‰å…¨è°ƒç”¨æ¨¡å‹"""
    try:
        response = model.invoke(prompt)
        if isinstance(response.content, str):
            try:
                return json.loads(response.content)
            except:
                return default_response or {"error": "è§£æå¤±è´¥"}
        return response.content
    except Exception as e:
        print(f"âš ï¸ è°ƒç”¨å‡ºé”™: {str(e)}")
        return default_response or {"error": str(e)}

def analyze_question(state: AgentState) -> AgentState:
    """åˆ†æç”¨æˆ·é—®é¢˜ï¼Œç¡®å®šéœ€è¦çš„ä¿¡æ¯ç±»å‹"""
    messages = state["messages"]
    last_message = messages[-1].content
    
    prompt = f"""
    åˆ†æä»¥ä¸‹é—®é¢˜å¹¶è¿”å› JSON æ ¼å¼ç»“æœï¼š
    é—®é¢˜ï¼š{last_message}
    
    {{"required_info": ["éœ€è¦çš„ä¿¡æ¯ç±»å‹"], "complexity": "é—®é¢˜å¤æ‚åº¦", "focus_area": "å…³æ³¨é¢†åŸŸ"}}
    """
    
    analysis = safe_invoke(prompt, {
        "required_info": ["basic"],
        "complexity": "simple",
        "focus_area": "general"
    })
    
    return {
        "messages": messages,
        "current_step": "analyze",
        "analysis": analysis,
        "next": "plan"
    }

def plan_response(state: AgentState) -> AgentState:
    """æ ¹æ®åˆ†æç»“æœè§„åˆ’å›ç­”ç­–ç•¥"""
    analysis = state["analysis"]
    
    prompt = f"""
    åŸºäºåˆ†æç»“æœè§„åˆ’å›ç­”ï¼š
    {json.dumps(analysis, ensure_ascii=False)}
    
    è¿”å› JSON æ ¼å¼ï¼š
    {{"steps": ["å›ç­”æ­¥éª¤"], "focus_points": ["é‡ç‚¹å†…å®¹"]}}
    """
    
    plan = safe_invoke(prompt, {
        "steps": ["direct_answer"],
        "focus_points": []
    })
    
    return {
        **state,
        "current_step": "plan",
        "analysis": {**analysis, "plan": plan},
        "next": "respond"
    }

def generate_response(state: AgentState) -> AgentState:
    """ç”Ÿæˆæœ€ç»ˆå›ç­”"""
    analysis = state["analysis"]
    messages = state["messages"]
    question = messages[-1].content
    
    prompt = f"""
    åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›ç­”ï¼š
    åˆ†æï¼š{json.dumps(analysis, ensure_ascii=False)}
    é—®é¢˜ï¼š{question}
    
    è¯·ç”Ÿæˆæ¸…æ™°ã€ä¸“ä¸šçš„å›ç­”ã€‚
    """
    
    response = model.invoke(prompt)
    
    return {
        "messages": [*messages, response],
        "current_step": "respond",
        "analysis": analysis,
        "next": "decide"
    }

def decide_next_step(state: AgentState) -> str:
    """å†³å®šä¸‹ä¸€æ­¥æ“ä½œ"""
    messages = state["messages"]
    current_step = state["current_step"]
    
    # å¦‚æœå·²ç»ç”Ÿæˆäº†å›ç­”ï¼Œå°±ç»“æŸå¯¹è¯
    if current_step == "respond":
        return "end"
    
    # å¦‚æœæ¶ˆæ¯ä¸­åŒ…å«"å†è§"ï¼Œä¹Ÿç»“æŸå¯¹è¯
    last_message = messages[-1]
    if isinstance(last_message.content, str) and "å†è§" in last_message.content:
        return "end"
        
    return "analyze"

# æ„å»ºå·¥ä½œæµå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("analyze", analyze_question)
workflow.add_node("plan", plan_response)
workflow.add_node("respond", generate_response)
workflow.add_node("end", lambda x: x)

# æ·»åŠ è¾¹å’Œæ¡ä»¶
workflow.add_edge("analyze", "plan")
workflow.add_edge("plan", "respond")
workflow.add_conditional_edges(
    "respond",
    decide_next_step,
    {
        "analyze": "analyze",
        "end": "end"
    }
)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("analyze")

# ç¼–è¯‘å›¾
graph = workflow.compile()

def test_conversation():
    """æµ‹è¯•å¤šé˜¶æ®µåˆ†æå¯¹è¯"""
    messages = [
        HumanMessage(content="è¯·åˆ†æå¼ ä¸‰çš„å­¦ä¹ æƒ…å†µï¼Œé‡ç‚¹å…³æ³¨ä»–çš„è¯¾ç¨‹æˆç»©å’Œå­¦ä¹ è¿›åº¦ã€‚")
    ]
    
    try:
        result = graph.invoke({
            "messages": messages,
            "current_step": "",
            "analysis": {},
            "next": "analyze"
        })
        
        print("\n=== å¯¹è¯è®°å½• ===")
        print("ğŸ¤” é—®é¢˜:", messages[0].content)
        print("\nğŸ” åˆ†æè¿‡ç¨‹:")
        print(json.dumps(result["analysis"], ensure_ascii=False, indent=2))
        print("\nğŸ’¡ å›ç­”:")
        print(result["messages"][-1].content)
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    test_conversation()